{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eiA_C8g4BAS"
      },
      "source": [
        "# Predicción de la supervivencia de pasajeros del desastre del Titanic con aprendizaje automático"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K4oG9qG4BAU"
      },
      "source": [
        "Este ejercicio tiene como objetivo predecir el resultado del desastre del Titanic utilizando aprendizaje automático. Los datos proporcionados están divididos en datos de entrenamiento y prueba. El archivo train.csv se utilizará para ajustar los algoritmos de aprendizaje automático que luego se aplicarán al archivo test.csv para predecir quién sobrevivirá. Los conjuntos de datos consisten en diferentes características que se enumeran a continuación:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuzUhKe94BAU"
      },
      "source": [
        "| Variable | Definición | Clave |\n",
        "| :- | -: | :-: |\n",
        "| PClass | Clase de Pasajero | 1=1ra, 2=2da, 3=3ra |\n",
        "| Sex | Género | |\n",
        "| Age | Edad en años |\n",
        "| SibSp | # de hermanos/as / cónyuges a bordo del Titanic  |\n",
        "| Parch | # de padres / hijos a bordo del Titanic |\n",
        "| Ticket | Número de boleto |\n",
        "| Fare | Tarifa del pasajero |\n",
        "| Cabin | Número de cabina |\n",
        "| Embarked | Puerto de embarque | C = Cherburgo, Q = Queenstown, S = Southampton |\n",
        "| Survived | Supervivencia | 0=No, 1=Sí |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y4Jz1SZ4BAU"
      },
      "source": [
        "Notas sobre las variables\n",
        "\n",
        "pclass:\n",
        "1ra = Superior,\n",
        "2da = Media,\n",
        "3ra = Inferior\n",
        "\n",
        "age: La edad es fraccionaria si es menor a 1. Si la edad es estimada, tendrá la forma xx.5\n",
        "\n",
        "sibsp: <br> El conjunto de datos define las relaciones familiares de esta manera: <br>\n",
        "Hermano/a = hermano, hermana, hermanastro, hermanastra <br>\n",
        "Cónyuge = esposo, esposa (amantes y prometidos/as fueron ignorados)\n",
        "\n",
        "parch: <br> El conjunto de datos define las relaciones familiares de esta manera: <br>\n",
        "Padre/Madre = madre, padre <br> Hijo/a = hija, hijo, hijastra, hijastro <br> Algunos niños viajaron solo con una niñera, por lo tanto parch=0 para ellos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHZgVMhR4BAV"
      },
      "source": [
        "Este ejercicio se divide en tres partes, siendo el procesamiento de datos la más larga e importante.\n",
        "1. Visualización de datos\n",
        "2. Procesamiento de datos\n",
        "3. Algoritmos de aprendizaje automático"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPvtSTHA4BAV"
      },
      "source": [
        "## Importar bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFx7f-fh4BAV"
      },
      "source": [
        "En esta primera parte hay que importar todas las librerías que se usarán. Seaborn y matplotlib se usan para la visualización de datos. Pandas y numpy ayudarán con el procesamiento de datos y álgebra lineal. Finalmente, se importan las bases de datos para los diferentes algoritmos de aprendizaje automático desde sklearn. Opcionalmente, se agrega una base de datos de imágenes con el propósito de añadir imágenes al cuaderno de Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4qFizNX4BAV"
      },
      "outputs": [],
      "source": [
        "# data visualization\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "# data processing\n",
        "import pandas as pd\n",
        "\n",
        "# linear algebra\n",
        "import numpy as np\n",
        "\n",
        "# Algorithms\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#Show images\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlnXJ6wG4BAW"
      },
      "source": [
        "# 1. Visualización de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3x9Fy5k4BAW"
      },
      "source": [
        "La primera parte de visualización de datos está para obtener un entendimiento del conjunto de datos. Solo conociendo con qué estamos trabajando, podemos determinar qué datos necesitamos o no.\n",
        "\n",
        "Con todas las librerías importadas, podemos comenzar creando nuevas variables \"test_df\" que contendrá nuestro conjunto de prueba y \"train_df\" que incluye el conjunto de entrenamiento. Para hacer esto, usamos la función \"read_csv\" de pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSk5UBFb4BAW"
      },
      "outputs": [],
      "source": [
        "#Loading the datasets\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_df = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4S1ZRdL4BAW"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3j0wOz64BAX"
      },
      "source": [
        "La función .info() en Python muestra información sobre el conjunto de datos. Muestra todas las columnas, el recuento de entradas dentro de las columnas y qué tipo tienen. En este caso, tenemos dos float64, cinco int64 y cinco objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Z_iCfJ4BAX"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwAu4j744BAX"
      },
      "outputs": [],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4erUHOix4BAX"
      },
      "source": [
        "Al echar un vistazo rápido al conjunto de entrenamiento, se pueden notar algunos problemas. Hay entradas __faltantes__ (NaN = not a number) que deben ser tratadas. Además, muchas entradas __no son numéricas__, lo que significa que deben ser convertidas __en valores numéricos__ para los algoritmos de aprendizaje automático."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLPQ2-lL4BAX"
      },
      "source": [
        "Veamos más de cerca qué es lo que realmente falta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAjxG9ik4BAY"
      },
      "outputs": [],
      "source": [
        "#Check for NaN or empty entries in trainingsdata\n",
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfAqRfIf4BAY"
      },
      "source": [
        "Con solo dos valores faltantes, la característica Embarked puede completarse fácilmente. La característica Age no es tan simple de completar y la característica Cabin con 687 valores faltantes probablemente será eliminada por completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_AOitsA4BAY"
      },
      "outputs": [],
      "source": [
        "print(train_df.columns.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj63jXzN4BAY"
      },
      "source": [
        "Pregunta: <br>\n",
        "Mirando nuevamente todas las características proporcionadas, ¿cuáles crees que no tendrán un efecto en los modelos de predicción?\n",
        "\n",
        "Respuesta: <br>\n",
        "PassengerId, Ticket, Name <br>\n",
        "\n",
        "Comencemos por mirar las otras características. (Primero Age y Sex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-znACti4BAY"
      },
      "source": [
        "## Age y Sex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLJCZBur4BAY"
      },
      "source": [
        "Para la visualización gráfica de Age y Sex con respecto a la tasa de supervivencia, usaremos un histograma. Un histograma muestra la distribución de una característica específica. En este caso, es la distribución de pasajeros femeninos y masculinos respecto a su edad y cuántos sobrevivieron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6irctKJC4BAY"
      },
      "outputs": [],
      "source": [
        "survived = 'survived'\n",
        "not_survived = 'not survived'\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "women = train_df[train_df['Sex']=='female']\n",
        "men = train_df[train_df['Sex']=='male']\n",
        "\n",
        "# Gráfico para mujeres\n",
        "ax = sns.histplot(women[women['Survived']==1].Age.dropna(), bins=18, label=survived, ax=axes[0], kde=False)\n",
        "ax = sns.histplot(women[women['Survived']==0].Age.dropna(), bins=40, label=not_survived, ax=axes[0], kde=False)\n",
        "ax.legend()\n",
        "ax.set_title('Female')\n",
        "\n",
        "# Gráfico para hombres\n",
        "ax = sns.histplot(men[men['Survived']==1].Age.dropna(), bins=18, label=survived, ax=axes[1], kde=False)\n",
        "ax = sns.histplot(men[men['Survived']==0].Age.dropna(), bins=40, label=not_survived, ax=axes[1], kde=False)\n",
        "ax.legend()\n",
        "_ = ax.set_title('Male')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBzXTKIz4BAZ"
      },
      "source": [
        "Al crear el histograma, se aprecia claramente que los hombres entre 20-40 años tuvieron una mayor probabilidad de supervivencia. Para las mujeres, la mayor probabilidad de supervivencia está entre 14 y 40 años. También es notable que los infantes tienen una tasa de supervivencia más alta.\n",
        "\n",
        "Pregunta: <br>\n",
        "¿Qué se puede hacer con la edad para simplificarla aún más?\n",
        "\n",
        "Respuesta: <br>\n",
        "Ciertas edades tienen mayores probabilidades de supervivencia que otras -> crear grupos para tener una escala similar a otras características."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTQ0NnY4BAZ"
      },
      "source": [
        "## Embarked, Pclass y Sex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fYsW4pO4BAZ"
      },
      "source": [
        "Para visualizar la característica Embarked, usaremos FacetGrid. Ayuda a mostrar distribuciones o relaciones entre múltiples variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAeGH2Yh4BAZ"
      },
      "outputs": [],
      "source": [
        "FacetGrid = sns.FacetGrid(train_df, row='Embarked', height=4.5, aspect=1.6)\n",
        "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep', order=None, hue_order=None)\n",
        "FacetGrid.add_legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCClnVbm4BAZ"
      },
      "source": [
        "La característica Embarked también muestra tener un gran impacto en la probabilidad de supervivencia de los pasajeros. Los hombres que embarcaron en el puerto C tienen una mayor probabilidad de supervivencia que en el puerto S o Q. Las mujeres, por otro lado, tienen una baja tasa de supervivencia en el puerto C y altas probabilidades en los puertos S y Q.\n",
        "La clase también parece tener un efecto en la tasa de supervivencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq2WFIyh4BAZ"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='Pclass', y='Survived', data=train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtw8MNrN4BAZ"
      },
      "source": [
        "Es inmediatamente evidente que la clase 1 tiene la mayoría de los sobrevivientes y la clase 3, la menor cantidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjt7H71B4BAZ"
      },
      "outputs": [],
      "source": [
        "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\n",
        "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
        "grid.add_legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crwUuF54BAa"
      },
      "source": [
        "Esta gráfica muestra la influencia de Pclass y destaca la alta tasa de mortalidad de los pasajeros en la clase 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bHjV7AT4BAa"
      },
      "source": [
        "## SibSp y Parch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB54yfcc4BAa"
      },
      "source": [
        "SibSp y Parch son características que muestran a pasajeros y cuántos familiares tienen a bordo. En consecuencia, esta característica debería combinarse en una sola. El siguiente código crea una nueva característica \"not_alone\" que muestra si un pasajero no está solo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8GZOvhN4BAa"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n",
        "    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n",
        "    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
        "    dataset['not_alone'] = dataset['not_alone'].astype(int)\n",
        "\n",
        "train_df['not_alone'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-bUJNEi4BAa"
      },
      "outputs": [],
      "source": [
        "axes = sns.pointplot(x='relatives', y='Survived', data=train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPkhYulf4BAa"
      },
      "source": [
        "La gráfica muestra la tasa de supervivencia en relación con la cantidad de familiares que tiene una persona. Los pasajeros con 1-3 familiares tienen mayores probabilidades de supervivencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPP24puA4BAa"
      },
      "source": [
        "__Concluyendo la visualización de datos:__\n",
        "- Buscamos valores vacíos o faltantes (NaN). Deben tratarse después.\n",
        "    - Cabin 687\n",
        "    - Age 177\n",
        "    - Embarked 2\n",
        "- Buscamos valores que tienen una influencia en la probabilidad de supervivencia.\n",
        "    - Age y Sex ambos tienen impacto en la supervivencia.\n",
        "    - Embarked y Pclass también muestran efecto en la supervivencia.\n",
        "- SibSp y Parch son características similares y deberían combinarse, y también parecen afectar la probabilidad de supervivencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFqjU_Gj4BAm"
      },
      "source": [
        "# 2. Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrGReCt-4BAm"
      },
      "source": [
        "Después de establecer una comprensión de los datos, podemos comenzar a darles la forma que necesitamos. Para los algoritmos de aprendizaje automático, queremos que todo esté en valores numéricos y que sea similar en escala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8foKrsFq4BAm"
      },
      "source": [
        "## PassengerId"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v9cZzaZ4BAm"
      },
      "source": [
        "El PassengerId no tiene influencia en la probabilidad de supervivencia. En este caso, eliminaremos la columna del conjunto de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD0KPB4J4BAn"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(['PassengerId'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qytcu9ay4BAn"
      },
      "outputs": [],
      "source": [
        "#Check tickets\n",
        "train_df['Ticket'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayy1ZFgG4BAn"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(['Ticket'], axis=1)\n",
        "test_df = test_df.drop(['Ticket'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfuJ3LA4BAn"
      },
      "source": [
        "## Manejo de datos faltantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTfwlBvc4BAn"
      },
      "source": [
        "### Cabin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzrtvStS4BAn"
      },
      "source": [
        "Mirando de nuevo las características, tenemos 687 valores faltantes en Cabin, 177 valores en Age y solo 2 en Embarked.\n",
        "\n",
        "Los valores en Cabin están compuestos por una letra y un número, siendo la letra la cubierta. En lugar de eliminar la característica por completo, solo eliminaremos el número y crearemos una nueva característica llamada \"deck\", resultante de la letra de Cabin. La letra de la cubierta se convertirá en numérica y los valores faltantes serán 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE-VQpTN4BAo"
      },
      "outputs": [],
      "source": [
        "#Antigua característica 'Cabin'\n",
        "print(train_df['Cabin'].head(8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXcQx-a04BAo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
        "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
        "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
        "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
        "    dataset['Deck'] = dataset['Deck'].astype(int)\n",
        "\n",
        "# we can now drop the cabin feature\n",
        "train_df = train_df.drop(['Cabin'], axis=1)\n",
        "test_df = test_df.drop(['Cabin'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuYkF2Tx4BAo"
      },
      "source": [
        "(Comparación entre Cabin y Deck)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ4awAK54BAo"
      },
      "outputs": [],
      "source": [
        "#Nueva característica 'Deck'\n",
        "print(dataset['Deck'].head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_aMLK4m4BAo"
      },
      "source": [
        "### Age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXhEljB4BAo"
      },
      "source": [
        "Para Age, se agregarán nuevos valores derivados de la edad media de la desviación estándar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cShasBL14BAp"
      },
      "outputs": [],
      "source": [
        "# Solución corregida para imputar edades faltantes en Titanic\n",
        "data = [train_df, test_df]\n",
        "\n",
        "# Se procesa cada conjunto (train y test) por separado\n",
        "for dataset in data:\n",
        "    # Se calcula la media y desviación estándar del MISMO conjunto que se va a modificar\n",
        "    mean = dataset[\"Age\"].mean()\n",
        "    std = dataset[\"Age\"].std()\n",
        "    is_null = dataset[\"Age\"].isnull().sum()\n",
        "\n",
        "    # Se generan valores aleatorios según la distribución del conjunto actual\n",
        "    rand_age = np.random.randint(mean - std, mean + std, size=is_null)\n",
        "\n",
        "    # Se crea una copia para evitar SettingWithCopyWarning\n",
        "    age_slice = dataset[\"Age\"].copy()\n",
        "\n",
        "    # Se reemplaza los valores NaN con los aleatorios generados\n",
        "    age_slice[np.isnan(age_slice)] = rand_age\n",
        "\n",
        "    # Se asignan de vuelta al dataset y convertimos a entero\n",
        "    dataset[\"Age\"] = age_slice.astype(int)\n",
        "\n",
        "# Se verifica que no quedan valores nulos en train_df\n",
        "print(\"Valores nulos en Age de train_df:\", train_df[\"Age\"].isnull().sum())\n",
        "print(\"Valores nulos en Age de test_df:\", test_df[\"Age\"].isnull().sum())\n",
        "\n",
        "# ALTERNATIVA 1: Eliminar filas con valores faltantes\n",
        "# train_df_clean = train_df.dropna(subset=[\"Age\"])\n",
        "# test_df_clean = test_df.dropna(subset=[\"Age\"])\n",
        "# Ventaja: Mantiene la pureza de los datos originales\n",
        "# Desventaja: Reduce significativamente el tamaño de la muestra\n",
        "\n",
        "# ALTERNATIVA 2: Imputación por grupos demográficos\n",
        "# Por ejemplo, usando título (Mr, Mrs, etc) y clase (Pclass)\n",
        "# train_df[\"Age\"] = train_df.groupby([\"Sex\", \"Pclass\"])[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
        "# Ventaja: Preserva mejor las relaciones entre variables\n",
        "# Desventaja: Mayor complejidad de implementación\n",
        "\n",
        "# ALTERNATIVA 3: Usar algoritmos de imputación avanzados\n",
        "# from sklearn.impute import KNNImputer\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# train_df[\"Age\"] = imputer.fit_transform(train_df[[\"Age\"]])\n",
        "# Ventaja: Mayor precisión estadística\n",
        "# Desventaja: Requiere normalización previa de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g69OJW7t4BAp"
      },
      "outputs": [],
      "source": [
        "print(train_df['Age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSeU7A14BAp"
      },
      "source": [
        "La característica Embarked tiene dos valores faltantes. Estos se completarán con la característica más común. La función .describe() de Python cuenta todos los valores y devuelve la función más importante. Esto aplica solo a valores categóricos. Para valores numéricos, la función .describe() devuelve percentiles y valores medios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW61Citq4BAp"
      },
      "outputs": [],
      "source": [
        "train_df['Embarked'].describe()\n",
        "# .describe funciona de manera diferente para series numéricas y categóricas. La siguiente vista sólo se logra con datos categóricos. Si más abajo\n",
        "# los valores se convierten a numéricos, se calcula el promedio, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh7kEzC54BAp"
      },
      "source": [
        "Para la característica Embarked, la más común es el puerto S, con 644 entradas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NblzQX1w4BAp"
      },
      "outputs": [],
      "source": [
        "common_value = 'S'\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYbYJOO34BAp"
      },
      "source": [
        "Después de llenar todos los valores faltantes, podemos verificar nuestro conjunto de datos con .info(). Todas las características deberían contener ahora 891 entradas y cero valores faltantes. <br>También podemos detectar las características recién añadidas \"not_alone\" y \"Deck\". La imagen importada muestra el conjunto de datos original sin el procesamiento de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yumojrJ84BAq"
      },
      "outputs": [],
      "source": [
        "#train_df.info()\n",
        "#print(\"\")\n",
        "#img = Image.open('train_df_info.jpg')\n",
        "#img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DbYJ4bQ4BAq"
      },
      "source": [
        "Como se mencionó anteriormente, todas las características deberían ser valores numéricos. El siguiente paso muestra la conversión a valores numéricos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JraRo44BAq"
      },
      "outputs": [],
      "source": [
        "#Converting \"fare\" from float64 to int 64\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SgXRsvB4BAq"
      },
      "source": [
        "La parte complicada de los nombres es que hay muchos títulos diferentes, aparte de Mr o Mrs, como \"Lady\", \"Countess\" y así sucesivamente. Estos no son numerosos pero aún deben tenerse en cuenta. Los siguientes títulos 'Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer' y 'Dona' se convertirán en un solo título llamado 'Rare'.\n",
        "\n",
        "'Mlle' y 'Ms' se convertirán en 'Miss' y 'Mme' será 'Mrs'.\n",
        "\n",
        "Al final, nos quedan cinco nuevos títulos en total -> \"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vatST0k-4BAq"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "\n",
        "for dataset in data:\n",
        "    # extract titles\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    # replace titles with a more common title or as Rare\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
        "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    # convert titles into numbers\n",
        "    dataset['Title'] = dataset['Title'].map(titles)\n",
        "    # filling NaN with 0, to get safe\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "\n",
        "train_df = train_df.drop(['Name'], axis=1)\n",
        "test_df = test_df.drop(['Name'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AToQyf2G4BAq"
      },
      "source": [
        "Nuevamente usando la función .map(), definimos male como 0 y female como 1. <br>\n",
        "Lo mismo se hace para convertir los puertos S, C y Q de la característica Embarked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ekx0iND4BAq"
      },
      "outputs": [],
      "source": [
        "genders = {\"male\": 0, \"female\": 1}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Sex'] = dataset['Sex'].map(genders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwOPds_P4BAq"
      },
      "outputs": [],
      "source": [
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(ports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFbfq4vj4BAr"
      },
      "source": [
        "Después de que todos los valores se han convertido a numéricos, nos queda el siguiente conjunto de datos de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzGkt5hb4BAr"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsRLGX-T4BAr"
      },
      "source": [
        "## Creación de nuevas categorías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApMu6owB4BAr"
      },
      "source": [
        "Referencia a 1. visualización de datos \"Age and Sex\" -> Crear nueva categoría <br>\n",
        "Pregunta: <br>\n",
        "¿Qué sería importante al crear nuevas clases? <br>\n",
        "\n",
        "Respuesta: <br>\n",
        "Dividirlas en grupos uniformes, para que ningún grupo incluya demasiadas características en comparación con los otros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhobUeYE4BAr"
      },
      "outputs": [],
      "source": [
        "test = pd.qcut(train_df['Age'], 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzEPUhoA4BAs"
      },
      "outputs": [],
      "source": [
        "test.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqPY79594BAs"
      },
      "outputs": [],
      "source": [
        "#Creating Categories\n",
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 21), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 21) & (dataset['Age'] <= 24), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 24) & (dataset['Age'] <= 28), 'Age'] = 3\n",
        "    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 32), 'Age'] = 4\n",
        "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 37), 'Age'] = 5\n",
        "    dataset.loc[(dataset['Age'] > 37) & (dataset['Age'] <= 45), 'Age'] = 6\n",
        "    dataset.loc[ dataset['Age'] > 45, 'Age'] = 7\n",
        "\n",
        "# let's see how it's distributed\n",
        "train_df['Age'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AQHDiDt4BAs"
      },
      "source": [
        "Pregunta: <br>\n",
        "¿Qué característica queda que también podría usar nuevas categorías? <br>\n",
        "\n",
        "Respuesta: <br>\n",
        "Fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LREL2P4f4BAs"
      },
      "outputs": [],
      "source": [
        "train_df['Fare'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loF_x2ll4BAt"
      },
      "source": [
        "Usando la función qcut(), pandas puede crear grupos de rangos que tienen cantidades similares de entradas. <br>\n",
        "Después de eso, podemos recorrer los datos y asignar el grupo a cada valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtZAdFZ4BAt"
      },
      "outputs": [],
      "source": [
        "groups = pd.qcut(train_df['Fare'], 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpfuVQ9R4BAt"
      },
      "outputs": [],
      "source": [
        "groups.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yjISMZg4BAt"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset.loc[ dataset['Fare'] <= 7, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7) & (dataset['Fare'] <= 8), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 8) & (dataset['Fare'] <= 14), 'Fare']   = 2\n",
        "    dataset.loc[(dataset['Fare'] > 14) & (dataset['Fare'] <= 26), 'Fare']   = 3\n",
        "    dataset.loc[(dataset['Fare'] > 26) & (dataset['Fare'] <= 52), 'Fare']   = 4\n",
        "    dataset.loc[ dataset['Fare'] > 52, 'Fare'] = 5\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIpLlq5r4BAt"
      },
      "outputs": [],
      "source": [
        "train_df['Fare'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLf5PCyQ4BAu"
      },
      "source": [
        "El último paso antes de aplicar los algoritmos de aprendizaje automático es crear dos nuevas características. <br>\n",
        "Pregunta: ¿cuáles se podrían combinar? (¿o es demasiado difícil?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "val6emPI4BAu"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age_Class']= dataset['Age']* dataset['Pclass']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2EOVy984BAu"
      },
      "outputs": [],
      "source": [
        "for dataset in data:\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)# Let's take a last look at the training set, before we start training the models.\n",
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7-QBJwB4BAu"
      },
      "source": [
        "# 3. Algoritmos de Aprendizaje de Máquinas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXBnweQn4BAu"
      },
      "outputs": [],
      "source": [
        "## Eliminar la característica \"Survived\" del conjunto de datos de entrenamiento y usar el conjunto de datos resultante como \"Predictor\", llámalo X_train.\n",
        "## Tomar la característica \"Survived\" del conjunto de datos de entrenamiento como \"Respuesta\" para tus modelos, llámala Y_train.\n",
        "\n",
        "# X_train = \"_____________(\"Survived\", axis=1)\"\n",
        "X_train = train_df.drop(\"Survived\", axis=1).copy()\n",
        "# Y_train = \"_____________\"\n",
        "Y_train = train_df[\"Survived\"].copy()\n",
        "\n",
        "## ¡Esto también lo puedes hacer! Borra el #\n",
        "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6wD_4_W4BAu"
      },
      "source": [
        "## Árbol de decisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTNUMWey4BAv"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo de árbol de decisión al conjunto de datos de entrenamiento\n",
        "\n",
        "## Queremos evaluar la puntuación de cada modelo de predicción, también es suficiente redondear la puntuación a dos decimales\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, Y_train)\n",
        "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
        "print(\"Decision Tree Accuracy: \", acc_decision_tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDmmfXMy4BAv"
      },
      "source": [
        "## Descenso de gradiente estocástico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXgmhRID4BAv"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo SGD al conjunto de datos de entrenamiento con un número máximo de iteraciones = 5 y tol = None\n",
        "sgd = SGDClassifier(max_iter=5, tol=None)\n",
        "sgd.fit(X_train, Y_train)\n",
        "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
        "print(\"SGD Accuracy: \", acc_sgd)\n",
        "\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4FGhMRd4BAv"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcdccY-I4BAv"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo Random Forest al conjunto de datos de entrenamiento con un número máximo de estimadores = 100\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100)\n",
        "random_forest.fit(X_train, Y_train)\n",
        "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
        "print(\"Random Forest Accuracy: \", acc_random_forest)\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByuVC5-s4BAv"
      },
      "source": [
        "## Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xogKx1Gf4BAw"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo de Regresión Logística al conjunto de datos de entrenamiento\n",
        "\n",
        "logreg = LogisticRegression(max_iter=2000)\n",
        "logreg.fit(X_train, Y_train)\n",
        "acc_logreg = round(logreg.score(X_train, Y_train) * 100, 2)\n",
        "print(\"Logistic Regression Accuracy: \", acc_logreg)\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVsuUvwL4BAw"
      },
      "source": [
        "## K vecinos más cercanos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72oaJNO64BAw"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo KNN al conjunto de datos de entrenamiento con 3 vecinos\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, Y_train)\n",
        "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
        "print(\"KNN Accuracy: \", acc_knn)\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfmwNAN24BAw"
      },
      "source": [
        "Bayes ingenuo gaussiano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY36K8xn4BAw"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo Bayes ingenuo gaussiano al conjunto de datos de entrenamiento\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, Y_train)\n",
        "acc_gnb = round(gnb.score(X_train, Y_train) * 100, 2)\n",
        "print(\"Naive Bayes Accuracy: \", acc_gnb)\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjINTL794BAw"
      },
      "source": [
        "## Perceptrón"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_qZAI2D4BAx"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo de Perceptrón al conjunto de datos de entrenamiento con 5 iteraciones\n",
        "\n",
        "perceptron = Perceptron(max_iter=5, tol=None)\n",
        "perceptron.fit(X_train, Y_train)\n",
        "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
        "print(\"Perceptron Accuracy: \", acc_perceptron)\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4yE-Hn94BAx"
      },
      "source": [
        "## Máquina de soporte vectorial lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4mGKKCE4BAx"
      },
      "outputs": [],
      "source": [
        "## Aplica el algoritmo de Máquina de soporte vectorial lineal al conjunto de datos de entrenamiento con 5 iteraciones\n",
        "\n",
        "linear_svc = SVC(max_iter=5, tol=None)\n",
        "linear_svc.fit(X_train, Y_train)\n",
        "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
        "print(\"Linear SVC Accuracy: \", acc_linear_svc)\n",
        "\n",
        "## Evalúa la puntuación como para el Árbol de decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8mPeUP94BAx"
      },
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SteAOBr84BAx"
      },
      "outputs": [],
      "source": [
        "## Al final de nuestro primer ejemplo completo de Aprendizaje Automático, queremos saber qué modelo se ajusta mejor a nuestro problema.\n",
        "## Por lo tanto, cambiar, si es necesario, el código de acuerdo con tus variables dadas para la puntuación\n",
        "\n",
        "results = pd.DataFrame({\n",
        "   'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',\n",
        "             'Random Forest', 'Naive Bayes', 'Perceptron',\n",
        "             'Stochastic Gradient Decent',\n",
        "             'Decision Tree'],\n",
        "   'Score': [acc_linear_svc, acc_knn, acc_log,\n",
        "             acc_random_forest, acc_gaussian, acc_perceptron,\n",
        "             acc_sgd, acc_decision_tree]})\n",
        "result_df = results.sort_values(by='Score', ascending=False)\n",
        "result_df = result_df.set_index('Score')\n",
        "result_df.head(9)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FHWS vLab Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
